<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="generator" content="pandoc" />
  <meta name="author" content="Romain Tavenard" />
  <title>Differentiability of DTW and the case of softDTW</title>
  <link rel="stylesheet" href="https://latex.now.sh/style.css">
  <link rel="stylesheet" href="prism/prism.css">
  <script src="prism/prism.js"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <style type="text/css">
    body {
      counter-reset: fignumber sidenote-counter theorem definition;
    }
    div#back2index {text-align: right;}
    h1#toctitle {text-align: left;}
    #TOC ul {list-style-type: none;}
    h1:not(.title) {margin-top: 1.625rem;}
    img {display: inline;}
    figure {text-align: center;}
    figcaption::before {
      counter-increment: fignumber;
      content: 'Figure ' counter(fignumber) '. ';
      font-weight: bold;
    }
    .csl-entry {
      clear: left;
      margin-bottom: 1em;
    }
    .csl-left-margin {
      float: left;
      padding-right: .5em;
      text-align: right;
      width: 5em;
    }
    .csl-right-inline {
      margin: 0 .4em 0 5.5em;
    }
    .theorem {
        background-color: #eee;
        border-radius: .5em;
        padding: .2em 1em;
    }
  </style>
</head>
<body>
<div id="header">
<h1 class="title">Differentiability of DTW and the case of softDTW</h1>
<h2 class="author">Romain Tavenard</h2>
<!-- <h3 class="date">2021/04/29</h3> -->
<div id="back2index">
  [<a href="index.html">Other blog posts</a>]
</div>
</div>
<div id="TOC">
  <h1 id="toctitle">Contents</h1>
  <ul>
  <li><a href="#introduction">Introduction</a></li>
  <li><a href="#differentiability-of-dtw">Differentiability of DTW</a></li>
  <li><a href="#softdtw-and-variants">softDTW and variants</a></li>
  <li><a href="#bibliography">References</a></li>
  </ul>
</div>
<h1 id="introduction">Introduction</h1>
<p>We have seen in a <a href="dtw.html">previous blog post</a> how one can use Dynamic Time Warping (DTW) as a shift-invariant similarity measure between time series. In this new post, we will study some aspects related to the differentiability of DTW. The reason why we focus on differentiability is that this property is key in modern machine learning approaches.</p>
<p><span class="citation" data-cites="cuturi2017soft">[<a href="#ref-cuturi2017soft" role="doc-biblioref">CuBl17</a>]</span> provide a nice example setting in which differentiability is desirable: Suppose we are given a forecasting task<label for="sn-1" class="sidenote-toggle sidenote-number"></label> <input type="checkbox" id="sn-1" class="sidenote-toggle" /> <span class="sidenote">A forecasting task is a task in which we are given the beginning of a time series and the goal is to predict the future behavior of the series.</span>in which the exact temporal localization of the temporal motifs to be predicted are less important than their overall shapes. In such a setting, it would make sense to use a shift-invariant similarity measure in order to assess whether a prediction made by the model is close enough from the ground-truth. Hence, a rather reasonable approach could be to tune the parameters of a neural network in order to minimize such a loss. Since optimization for this family of models heavily relies on gradient descent, having access to a differentiable shift-invariant similarity measure between time series is a key ingredient of this approach.</p>
<h1 id="differentiability-of-dtw">Differentiability of DTW</h1>
<p>Let us start by having a look at the differentiability of Dynamic Time Warping. To do so, we will rely on the following theorem from <span class="citation" data-cites="bonnans1998optimization">[<a href="#ref-bonnans1998optimization" role="doc-biblioref">BoSh98</a>]</span>:</p>
<div class="theorem">
<p>Let <span class="math inline">\(\Phi\)</span> be a metric space, <span class="math inline">\(X\)</span> be a normed space, and <span class="math inline">\(\Pi\)</span> be a compact subset of <span class="math inline">\(\Phi\)</span>. Let us define the optimal value function <span class="math inline">\(v\)</span> as:</p>
<p><span class="math display">\[
  v(x) = \inf_{\pi \in \Pi} f(x ; \pi) \, .
\]</span></p>
<p>Suppose that:</p>
<ol type="1">
<li>for all <span class="math inline">\(\pi \in \Phi\)</span>, the function <span class="math inline">\(x \mapsto f( x ; \pi )\)</span> is differentiable;</li>
<li><span class="math inline">\(f(x ; \pi)\)</span> and <span class="math inline">\(D_x f(x ; \pi)\)</span> the derivative of <span class="math inline">\(x \mapsto f( x ; \pi )\)</span> are continuous on <span class="math inline">\(X \times \Phi\)</span>.</li>
</ol>
<p>If, for <span class="math inline">\(x^0 \in X\)</span>, <span class="math inline">\(\pi \mapsto f(x^0 ; \pi )\)</span> has a unique minimizer <span class="math inline">\(\pi^0\)</span> over <span class="math inline">\(\Pi\)</span> then <span class="math inline">\(v\)</span> is differentiable at <span class="math inline">\(x^0\)</span> and <span class="math inline">\(Dv(x^0) = D_x f(x^0 ; \pi^0)\)</span>.</p>
</div>
<p>Let us come back to Dynamic Time Warping, and suppose we are given a reference time series <span class="math inline">\(x_\text{ref}\)</span>. We would like to study the differentiability of</p>
<p><span class="math display">\[
\begin{aligned}
  v(x) &amp;= DTW_2(x, x_\text{ref}) \\
       &amp;= \min_{\pi \in \mathcal{A}(x, x_\text{ref})} \left\langle A_\pi , D_2(x, x_\text{ref}) \right\rangle^{\frac{1}{2}}
\end{aligned}
\]</span></p>
<p>then the previous Theorem tells us that <span class="math inline">\(v\)</span> is differentiable everywhere except when:</p>
<ul>
<li><span class="math inline">\(DTW_2(x, x_\text{ref}) = 0\)</span> since, in this case, the non-differentiability of the square root function breaks condition 1 of the Theorem above;</li>
<li>there exist several optimal paths for the DTW problem.</li>
</ul>
<p>This second condition is illustrated in the Figure below in which we vary the value of a single element in one of the time series (for visualization purposes) and study the evolution of <span class="math inline">\(DTW_2(x, x_\text{ref})\)</span> as a function of this value:</p>
<figure>
<video playsinline muted autoplay controls loop width="80%">
<source src="fig/dtw_landscape.webm" type="video/webm" />
<source src="fig/dtw_landscape.mp4" type="video/mp4" />
<img src="fig/dtw_landscape.gif" alt="DTW landscape" /> </video>
<figcaption>
(Non-)differentiability of Dynamic Time Warping.
</figcaption>
</figure>
<p>Note the sudden change in slope at the position marked by a vertical dashed line, which corresponds to a case where (at least) two distinct optimal alignment paths coexist.</p>
<h1 id="softdtw-and-variants">softDTW and variants</h1>
<ul>
<li>A discussion about soft-min</li>
<li>softDTW definition</li>
<li>soft alignment path
<ul>
<li>interpretation as the expectation across all alignments</li>
</ul></li>
<li>properties
<ul>
<li><span class="math inline">\(\gamma t^2\)</span> impact of a <span class="math inline">\(t\)</span>-offset (visu) [done]</li>
<li>denoising effect</li>
</ul></li>
<li>Visualization of a loss landscape for softDTW as a function of <span class="math inline">\(\gamma\)</span> [done]</li>
<li>sharp softDTW, divergences&#x2026;</li>
</ul>
<figure>
<video playsinline muted autoplay controls loop width="80%">
<source src="fig/softdtw_shift.webm" type="video/webm" />
<source src="fig/softdtw_shift.mp4" type="video/mp4" />
<img src="fig/softdtw_shift.gif" alt="Impact of time shifts on softDTW" /> </video>
<figcaption>
Impact of time shifts on softDTW.
</figcaption>
</figure>
<figure>
<video playsinline muted autoplay controls loop width="80%">
<source src="fig/softdtw_landscape.webm" type="video/webm" />
<source src="fig/softdtw_landscape.mp4" type="video/mp4" />
<img src="fig/softdtw_landscape.gif" alt="softDTW landscape" /> </video>
<figcaption>
Differentiability of softDTW.
</figcaption>
</figure>
<h1 class="unnumbered" id="bibliography">References</h1>
<div id="refs" class="references csl-bib-body" role="doc-bibliography">
<div id="ref-bonnans1998optimization" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[BoSh98] </div><div class="csl-right-inline"><span class="smallcaps">Bonnans, J Fr&#xE9;d&#xE9;ric</span> ; <span class="smallcaps">Shapiro, Alexander</span>: Optimization problems with perturbations: A guided tour. In: <em>SIAM review</em> Bd. 40, SIAM (1998), Nr.&#xA0;2, S.&#xA0;228&#x2013;264</div>
</div>
<div id="ref-cuturi2017soft" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[CuBl17] </div><div class="csl-right-inline"><span class="smallcaps">Cuturi, Marco</span> ; <span class="smallcaps">Blondel, Mathieu</span>: Soft-DTW: a differentiable loss function for time-series. In: <em>Proceedings of the International Conference on Machine Learning</em>, 2017, S.&#xA0;894&#x2013;903</div>
</div>
</div>
</body>
</html>
