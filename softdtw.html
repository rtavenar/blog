<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="generator" content="pandoc" />
  <meta name="author" content="Romain Tavenard" />
  <title>Differentiability of DTW and the case of soft-DTW</title>
  <link rel="stylesheet" href="https://latex.now.sh/style.css">
  <link rel="stylesheet" href="prism/prism.css">
  <script src="prism/prism.js"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <style type="text/css">
    body {
      counter-reset: fignumber sidenote-counter theorem definition;
    }
    div#back2index {text-align: right;}
    h1#toctitle {text-align: left;}
    #TOC ul {list-style-type: none;}
    h1:not(.title) {margin-top: 1.625rem;}
    img {display: inline;}
    figure {text-align: center;}
    figcaption::before {
      counter-increment: fignumber;
      content: 'Figure ' counter(fignumber) '. ';
      font-weight: bold;
    }
    .csl-entry {
      clear: left;
      margin-bottom: 1em;
    }
    .csl-left-margin {
      float: left;
      padding-right: .5em;
      text-align: right;
      width: 5em;
    }
    .csl-right-inline {
      margin: 0 .4em 0 5.5em;
    }
    .theorem {
        background-color: #eee;
        border-radius: .5em;
        padding: .2em 1em;
    }
  </style>
  
  <!-- pandoc-eqnos: equation style -->
  <style>
    .eqnos { display: inline-block; position: relative; width: 100%; }
    .eqnos br { display: none; }
    .eqnos-number { position: absolute; right: 0em; top: 50%; line-height: 0; }
  </style>
</head>
<body>
<div id="header">
<h1 class="title">Differentiability of DTW and the case of soft-DTW</h1>
<h2 class="author">Romain Tavenard</h2>
<!-- <h3 class="date">2021/04/29</h3> -->
<div id="back2index">
  [<a href="index.html">Other blog posts</a>]
</div>
</div>
<div id="TOC">
  <h1 id="toctitle">Contents</h1>
  <ul>
  <li><a href="#introduction">Introduction</a></li>
  <li><a href="#differentiability-of-dtw">Differentiability of DTW</a></li>
  <li><a href="#soft-dtw-and-variants">Soft-DTW and variants</a>
  <ul>
  <li><a href="#a-note-on-soft-min">A note on soft-min</a></li>
  <li><a href="#soft-alignment-path">Soft-Alignment Path</a></li>
  <li><a href="#properties">Properties</a></li>
  <li><a href="#related-similarity-measures">Related Similarity Measures</a></li>
  </ul></li>
  <li><a href="#conclusion">Conclusion</a></li>
  <li><a href="#bibliography">References</a></li>
  </ul>
</div>
<h1 id="introduction">Introduction</h1>
<p>We have seen in a <a href="dtw.html">previous blog post</a> how one can use Dynamic Time Warping (DTW) as a shift-invariant similarity measure between time series. In this new post, we will study some aspects related to the differentiability of DTW. The reason why we focus on differentiability is that this property is key in modern machine learning approaches.</p>
<p><span class="citation" data-cites="cuturi2017soft">[<a href="#ref-cuturi2017soft" role="doc-biblioref">CuBl17</a>]</span> provide a nice example setting in which differentiability is desirable: Suppose we are given a forecasting task<label for="sn-1" class="sidenote-toggle sidenote-number"></label> <input type="checkbox" id="sn-1" class="sidenote-toggle" /> <span class="sidenote">A forecasting task is a task in which we are given the beginning of a time series and the goal is to predict the future behavior of the series.</span>in which the exact temporal localization of the temporal motifs to be predicted are less important than their overall shapes. In such a setting, it would make sense to use a shift-invariant similarity measure in order to assess whether a prediction made by the model is close enough from the ground-truth. Hence, a rather reasonable approach could be to tune the parameters of a neural network in order to minimize such a loss. Since optimization for this family of models heavily relies on gradient descent, having access to a differentiable shift-invariant similarity measure between time series is a key ingredient of this approach.</p>
<h1 id="differentiability-of-dtw">Differentiability of DTW</h1>
<p>Let us start by having a look at the differentiability of Dynamic Time Warping. To do so, we will rely on the following theorem from <span class="citation" data-cites="bonnans1998optimization">[<a href="#ref-bonnans1998optimization" role="doc-biblioref">BoSh98</a>]</span>:</p>
<div class="theorem">
<p>Let <span class="math inline">\(\Phi\)</span> be a metric space, <span class="math inline">\(X\)</span> be a normed space, and <span class="math inline">\(\Pi\)</span> be a compact subset of <span class="math inline">\(\Phi\)</span>. Let us define the optimal value function <span class="math inline">\(v\)</span> as:</p>
<p><span class="math display">\[
  v(x) = \inf_{\pi \in \Pi} f(x ; \pi) \, .
\]</span></p>
<p>Suppose that:</p>
<ol type="1">
<li>for all <span class="math inline">\(\pi \in \Phi\)</span>, the function <span class="math inline">\(x \mapsto f( x ; \pi )\)</span> is differentiable;</li>
<li><span class="math inline">\(f(x ; \pi)\)</span> and <span class="math inline">\(D_x f(x ; \pi)\)</span> the derivative of <span class="math inline">\(x \mapsto f( x ; \pi )\)</span> are continuous on <span class="math inline">\(X \times \Phi\)</span>.</li>
</ol>
<p>If, for <span class="math inline">\(x^0 \in X\)</span>, <span class="math inline">\(\pi \mapsto f(x^0 ; \pi )\)</span> has a unique minimizer <span class="math inline">\(\pi^0\)</span> over <span class="math inline">\(\Pi\)</span> then <span class="math inline">\(v\)</span> is differentiable at <span class="math inline">\(x^0\)</span> and <span class="math inline">\(Dv(x^0) = D_x f(x^0 ; \pi^0)\)</span>.</p>
</div>
<p>Let us come back to Dynamic Time Warping, and suppose we are given a reference time series <span class="math inline">\(x_\text{ref}\)</span>. We would like to study the differentiability of</p>
<p><span class="math display">\[
\begin{aligned}
  v(x) &amp;= DTW_2(x, x_\text{ref}) \\
       &amp;= \min_{\pi \in \mathcal{A}(x, x_\text{ref})} \left\langle A_\pi , D_2(x, x_\text{ref}) \right\rangle^{\frac{1}{2}}
\end{aligned}
\]</span></p>
<p>then the previous Theorem tells us that <span class="math inline">\(v\)</span> is differentiable everywhere except when:</p>
<ul>
<li><span class="math inline">\(DTW_2(x, x_\text{ref}) = 0\)</span> since, in this case, the non-differentiability of the square root function breaks condition 1 of the Theorem above;</li>
<li>there exist several optimal paths for the DTW problem.</li>
</ul>
<p>This second condition is illustrated in the Figure below in which we vary the value of a single element in one of the time series (for visualization purposes) and study the evolution of <span class="math inline">\(DTW_2(x, x_\text{ref})\)</span> as a function of this value:</p>
<figure>
<video playsinline muted autoplay controls loop width="80%">
<source src="fig/dtw_landscape.webm" type="video/webm" />
<source src="fig/dtw_landscape.mp4" type="video/mp4" />
<img src="fig/dtw_landscape.gif" alt="DTW landscape" /> </video>
<figcaption>
(Non-)differentiability of Dynamic Time Warping.
</figcaption>
</figure>
<p>Note the sudden change in slope at the position marked by a vertical dashed line, which corresponds to a case where (at least) two distinct optimal alignment paths coexist.</p>
<h1 id="soft-dtw-and-variants">Soft-DTW and variants</h1>
<p>Soft-DTW <span class="citation" data-cites="cuturi2017soft">[<a href="#ref-cuturi2017soft" role="doc-biblioref">CuBl17</a>]</span> has been introduced as a way to mitigate this limitation. The formal definition for soft-DTW is the following:</p>
<p><span class="math display">\[\begin{equation}
\text{soft-}DTW^{\gamma}(x, x^\prime) =
    \min_{\pi \in \mathcal{A}(x, x^\prime)}{}^\gamma
        \sum_{(i, j) \in \pi} d(x_i, x^\prime_j)^2
\label{eq:softdtw}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\min{}^\gamma\)</span> is the soft-min operator parametrized by a smoothing factor <span class="math inline">\(\gamma\)</span>.</p>
<h2 id="a-note-on-soft-min">A note on soft-min</h2>
<p>The soft-min operator <span class="math inline">\(\min{}^\gamma\)</span> is defined as:</p>
<p><span class="math display">\[\begin{equation}
    \min{}^\gamma(a_1, \dots, a_n) = - \gamma \log \sum_i e^{-a_i / \gamma}
\end{equation}\]</span></p>
<p>Note that when gamma tends to <span class="math inline">\(0^+\)</span>, the term corresponding to the lower <span class="math inline">\(a_i\)</span> value will dominate other terms in the sum, and the soft-min then tends to the hard minimum, as illustrated below:</p>
<figure>
<img src="fig/soft_min.svg" alt="soft-min function" width="80%" />
<figcaption>
The soft-min function <span class="math inline">\(\min^\gamma\)</span> applied to the pair <span class="math inline">\((-a, a)\)</span> for various values of <span class="math inline">\(\gamma\)</span>.<label for="sn-softmin" class="sidenote-toggle sidenote-number"></label> <input type="checkbox" id="sn-softmin" class="sidenote-toggle" /> <span class="sidenote">This Figure is inspired from <a href="https://en.wikipedia.org/wiki/Smooth_maximum">the dedicated wikipedia page</a>.</span>
</figcaption>
</figure>
<p>As a consequence, we have:</p>
<p><span class="math display">\[\begin{equation}
    \text{soft-}DTW^{\gamma}(x, x^\prime)
    \xrightarrow{\gamma \to 0^+} DTW_2(x, x^\prime)^2 \, .
\end{equation}\]</span></p>
<p>However, contrary to DTW, soft-DTW is differentiable everywhere for strictly positive <span class="math inline">\(\gamma\)</span> even if, for small <span class="math inline">\(\gamma\)</span> values, sudden changes can still occur in the loss landscape, as seen in the Figure below:</p>
<figure>
<video playsinline muted autoplay controls loop width="80%">
<source src="fig/softdtw_landscape.webm" type="video/webm" />
<source src="fig/softdtw_landscape.mp4" type="video/mp4" />
<img src="fig/softdtw_landscape.gif" alt="softDTW landscape" /> </video>
<figcaption>
Differentiability of soft-DTW. For the sake of visualization, soft-DTW divergence, that is a normalized version of soft-DTW <a href="#related-similarity-measures">discussed below</a>, is reported in place of soft-DTW.
</figcaption>
</figure>
<p>Note that the recurrence relation we had in Eq. <a href="dtw.html#eq:rec">(2)</a> of the post on DTW is still valid with this <span class="math inline">\(\min^\gamma\)</span> formulation, hence the <span class="math inline">\(O(mn)\)</span> DTW algorithm is still valid here (the only difference being that soft-min should be used in the update rule in place of min).</p>
<h2 id="soft-alignment-path">Soft-Alignment Path</h2>
<p>It is shown in <span class="citation" data-cites="mensch2018">[<a href="#ref-mensch2018" role="doc-biblioref">MeBl18</a>]</span> that soft-DTW can be re-written:</p>
<div class="scroll-wrapper">
<p><span class="math display">\[\begin{equation}
\text{soft-}DTW^{\gamma}(x, x^\prime) =
    \min_{p \in \Sigma^{|\mathcal{A}(x, x^\prime)|}} \left\langle \sum_{\pi \in \mathcal{A}(x, x^\prime)} p(\pi) A_\pi , D_2(x, x^\prime) \right\rangle - \gamma H(p)
\end{equation}\]</span></p>
</div>
<p>where <span class="math inline">\(\Sigma^{|\mathcal{A}(x, x^\prime)|}\)</span> is the set of probability distributions over paths and <span class="math inline">\(H(p)\)</span> is the entropy of a given probability distribution <span class="math inline">\(p\)</span>. For strictly positive <span class="math inline">\(\gamma\)</span>, this problem has a closed-form solution that is:</p>
<p><span class="math display">\[
    p^\star_\gamma(\pi) = \frac{e^{-\langle A_\pi, D_2(x, x^\prime) / \gamma\rangle}}{k_{\mathrm{GA}}^{\gamma}(x, x^\prime)}
\]</span></p>
<p>where <span class="math inline">\(k_{\mathrm{GA}}^{\gamma}(x, x^\prime)\)</span> is the Global Alignment kernel <span class="citation" data-cites="cuturi2007kernel">[<a href="#ref-cuturi2007kernel" role="doc-biblioref">CVBM07</a>]</span> that acts as a normalization factor here.</p>
<p>This formulation leads to the following definition for the soft-alignment matrix <span class="math inline">\(A_\gamma\)</span></p>
<p><span id="eq:a_gamma" class="eqnos"><span class="math display">\[
    A_\gamma = \sum_{\pi \in \mathcal{A}(x, x^\prime)} p^\star_\gamma(\pi) A_\pi \, .
\]</span><span class="eqnos-number">(1)</span></span> </p>
<p><span class="math inline">\(A_\gamma\)</span> is a matrix that informs, for each pair <span class="math inline">\((i, j)\)</span>, how much it will be taken into account in the matching.</p>
<figure>
<video playsinline muted autoplay controls loop width="60%">
<source src="fig/a_gamma.webm" type="video/webm" />
<source src="fig/a_gamma.mp4" type="video/mp4" />
<img src="fig/a_gamma.gif" alt="$A_\gamma$ matrix" /> </video>
<figcaption>
<span class="math inline">\(A_\gamma\)</span> matrix. Note how the matrix blurs out when <span class="math inline">\(\gamma\)</span> grows.
</figcaption>
</figure>
<p>Note that when <span class="math inline">\(\gamma\)</span> tends toward <span class="math inline">\(+\infty\)</span>, then <span class="math inline">\(p^\star_\gamma\)</span> weights tend to the uniform distribution, hence the averaging operates over all alignments with equal weights, and the corresponding <span class="math inline">\(A_\infty\)</span> matrix tends to favor diagonal matches, regardless of the content of the series <span class="math inline">\(x\)</span> and <span class="math inline">\(x^\prime\)</span>:</p>
<figure>
<img src="fig/a_inf.svg" alt="$A_\infty$ matrix" width="60%" />
<figcaption>
<span class="math inline">\(A_\infty\)</span> matrix for time series of length 30.
</figcaption>
</figure>
<p>However, the sum in Eq.&#xA0;<a href="#eq:a_gamma">(1)</a> is intractable due to the very large number of paths in <span class="math inline">\(\mathcal{A}(x, x^\prime)\)</span>. Fortunately, once soft-DTW has been computed, <span class="math inline">\(A_\gamma\)</span> can be obtained through a backward dynamic programming pass with complexity <span class="math inline">\(O(mn)\)</span> (see more details in <span class="citation" data-cites="cuturi2017soft">[<a href="#ref-cuturi2017soft" role="doc-biblioref">CuBl17</a>]</span>).</p>
<p>Computing this <span class="math inline">\(A_\gamma\)</span> matrix is especially useful since it is directly related to the gradients of the soft-DTW similarity measure:</p>
<p><span class="math display">\[\begin{equation}
\nabla_{x} \text{soft-}DTW^{\gamma}(x, x^\prime) =
    \left(\frac{\partial D_2(x, x^\prime)}{\partial x} \right)^T A_\gamma \, .
\end{equation}\]</span></p>
<h2 id="properties">Properties</h2>
<p>As discussed in <span class="citation" data-cites="janati2020spatio">[<a href="#ref-janati2020spatio" role="doc-biblioref">JaCuGr20</a>]</span>, soft-DTW is not invariant to time shifts, as is DTW. Suppose <span class="math inline">\(x\)</span> is a time series that is constant except for a motif that occurs at some point in the series, and let us denote by <span class="math inline">\(x_{+k}\)</span> a copy of <span class="math inline">\(x\)</span> in which the motif is temporally shifted by <span class="math inline">\(k\)</span> timestamps. Then the quantity</p>
<div class="scroll-wrapper">
<p><span class="math display">\[\begin{equation*}
\Delta^\gamma(x, x_{+k}) = \left| \text{soft-}DTW^{\gamma}(x, x_{+k}) - \text{soft-}DTW^{\gamma}(x, x) \right|
\end{equation*}\]</span></p>
</div>
<p>grows linearly with <span class="math inline">\(\gamma k^2\)</span>:</p>
<figure>
<video playsinline muted autoplay controls loop width="80%">
<source src="fig/softdtw_shift.webm" type="video/webm" />
<source src="fig/softdtw_shift.mp4" type="video/mp4" />
<img src="fig/softdtw_shift.gif" alt="Impact of time shifts on soft-DTW" /> </video>
<figcaption>
Impact of time shifts on soft-DTW.
</figcaption>
</figure>
<p>The reason behind this sensibility to time shifts is that soft-DTW provides a weighted average similarity score across all alignment paths (where stronger weights are assigned to better paths), instead of focusing on the single best alignment as done in DTW.</p>
<p>Another important property of soft-DTW is that is has a &#x201C;denoising effect&#x201D;, in the sense that, for a given time series <span class="math inline">\(x_\text{ref}\)</span>, the minimizer of <span class="math inline">\(\text{soft-}DTW^{\gamma}(x, x_\text{ref})\)</span> is not <span class="math inline">\(x_\text{ref}\)</span> itself but rather a smoothed version of it:</p>
<figure>
<video playsinline muted autoplay controls loop width="100%">
<source src="fig/softdtw_denoising.webm" type="video/webm" />
<source src="fig/softdtw_denoising.mp4" type="video/mp4" />
<img src="fig/softdtw_denoising.gif" alt="Denoising effect of soft-DTW" /> </video>
<figcaption>
Denoising effect of soft-DTW. Here, we perform a gradient descent on <span class="math inline">\(x\)</span>, initialized at <span class="math inline">\(x^{(0)} = x_\text{ref}\)</span>. Note how using larger <span class="math inline">\(\gamma\)</span> values (right column) tends to smooth out more details.<label for="sn-denoising" class="sidenote-toggle sidenote-number"></label> <input type="checkbox" id="sn-denoising" class="sidenote-toggle" /> <span class="sidenote">This Figure is widely inspired from Figure 2 in <span class="citation" data-cites="blondelmensch2020">[<a href="#ref-blondelmensch2020" role="doc-biblioref">BlMeVe21</a>]</span>.</span>
</figcaption>
</figure>
<h2 id="related-similarity-measures">Related Similarity Measures</h2>
<p>In <span class="citation" data-cites="blondelmensch2020">[<a href="#ref-blondelmensch2020" role="doc-biblioref">BlMeVe21</a>]</span>, new similarity measures are defined, that rely on soft-DTW. In particular, <strong>soft-DTW divergence</strong> is defined as:</p>
<div class="scroll-wrapper">
<p><span class="math display">\[\begin{equation}
    D^\gamma (x, x^\prime) =
        \text{soft-}DTW^{\gamma}(x, x^\prime)
        - \frac{1}{2} \left(
                \text{soft-}DTW^{\gamma}(x, x) +
                \text{soft-}DTW^{\gamma}(x^\prime, x^\prime)
            \right)
\end{equation}\]</span></p>
</div>
<p>and this divergence has the advantage of being minimized for <span class="math inline">\(x = x^\prime\)</span> (and being exactly 0 in that case).</p>
<!-- Second, another interesting similarity measure introduced in the same paper is
the **sharp soft-DTW** which is:

\begin{equation}
    \text{sharp-soft-}DTW^{\gamma} (x, x^\prime) =
        \langle A_\gamma,  D_2(x, x^\prime) \rangle
\end{equation}

Note that a **sharp soft-DTW divergence** can be derived from this
(with a similar approach as for $D^\gamma$), which has the extra benefit
(over the sharp soft-DTW) of
being minimized at $x = x^\prime$.

Further note that, by pushing $\gamma$ to the $+\infty$ limit in this formula,
one gets:

\begin{equation}
\text{sharp-soft-}DTW^{\gamma}(x, x^\prime)
    \xrightarrow{\gamma \to +\infty}
    \left\langle A_\infty, D_2(x, x^\prime) \right\rangle \, ,
\end{equation}

where $A_\infty$ tends to favor diagonal matches:

<figure>
    <img src="fig/a_inf.svg" alt="$A_\infty$ matrix" width="60%" />
    <figcaption> 
        $A_\infty$ matrix for time series of length 30.
    </figcaption>
</figure> -->
<p>Also, in <span class="citation" data-cites="hadji2020">[<a href="#ref-hadji2020" role="doc-biblioref">HaDeJe21</a>]</span>, a variant of <span class="math inline">\(\min^\gamma\)</span>, called <span class="math inline">\(\text{smoothMin}^\gamma\)</span> is used in the recurrence formula. Contrary to <span class="math inline">\(\min^\gamma\)</span>, <span class="math inline">\(\text{smoothMin}^\gamma\)</span> upper bounds the min operator:</p>
<figure>
<img src="fig/smooth_min.svg" alt="smooth-min function" width="80%" />
<figcaption>
The smooth-min function <span class="math inline">\(\text{smoothMin}^\gamma\)</span> applied to the pair <span class="math inline">\((-a, a)\)</span> for various values of <span class="math inline">\(\gamma\)</span>.
</figcaption>
</figure>
<p>As a consequence, the resulting similarity measure upper bounds DTW. Note also that <span class="citation" data-cites="hadji2020">[<a href="#ref-hadji2020" role="doc-biblioref">HaDeJe21</a>]</span> suggest that the DTW variants presented in these posts are not fully suited for representation learning and additional contrastive losses should be used to help learn useful representations.</p>
<h1 id="conclusion">Conclusion</h1>
<p><strong>TODO</strong> Our next blog post should be dedicated to drawing links between optimal transport and dynamic time warping.</p>
<h1 class="unnumbered" id="bibliography">References</h1>
<div id="refs" class="references csl-bib-body" role="doc-bibliography">
<div id="ref-blondelmensch2020" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[BlMeVe21] </div><div class="csl-right-inline"><span class="smallcaps">Blondel, Mathieu</span> ; <span class="smallcaps">Mensch, Arthur</span> ; <span class="smallcaps">Vert, Jean-Philippe</span>: Differentiable Divergences Between Time Series. In: <em>Proceedings of the International Conference on Artificial Intelligence and Statistics</em>, <em>Proceedings of Machine Learning Research</em>. Bd. 130&#xA0;: PMLR, 2021, S.&#xA0;3853&#x2013;3861</div>
</div>
<div id="ref-bonnans1998optimization" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[BoSh98] </div><div class="csl-right-inline"><span class="smallcaps">Bonnans, J Fr&#xE9;d&#xE9;ric</span> ; <span class="smallcaps">Shapiro, Alexander</span>: Optimization problems with perturbations: A guided tour. In: <em>SIAM review</em> Bd. 40, SIAM (1998), Nr.&#xA0;2, S.&#xA0;228&#x2013;264</div>
</div>
<div id="ref-cuturi2017soft" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[CuBl17] </div><div class="csl-right-inline"><span class="smallcaps">Cuturi, Marco</span> ; <span class="smallcaps">Blondel, Mathieu</span>: Soft-DTW: a differentiable loss function for time-series. In: <em>Proceedings of the International Conference on Machine Learning</em>, 2017, S.&#xA0;894&#x2013;903</div>
</div>
<div id="ref-cuturi2007kernel" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[CVBM07] </div><div class="csl-right-inline"><span class="smallcaps">Cuturi, Marco</span> ; <span class="smallcaps">Vert, Jean-Philippe</span> ; <span class="smallcaps">Birkenes, Oystein</span> ; <span class="smallcaps">Matsui, Tomoko</span>: A kernel for time series based on global alignments. In: <em>Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing</em>. Bd. 2&#xA0;: IEEE, 2007, S.&#xA0;II&#x2013;413</div>
</div>
<div id="ref-hadji2020" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[HaDeJe21] </div><div class="csl-right-inline"><span class="smallcaps">Hadji, Isma</span> ; <span class="smallcaps">Derpanis, Konstantinos G</span> ; <span class="smallcaps">Jepson, Allan D</span>: Representation Learning via Global Temporal Alignment and Cycle-Consistency. In: <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>, 2021</div>
</div>
<div id="ref-janati2020spatio" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[JaCuGr20] </div><div class="csl-right-inline"><span class="smallcaps">Janati, Hicham</span> ; <span class="smallcaps">Cuturi, Marco</span> ; <span class="smallcaps">Gramfort, Alexandre</span>: Spatio-Temporal Alignments: Optimal transport through space and time. In: <em>Proceedings of the International Conference on Artificial Intelligence and Statistics</em>&#xA0;: PMLR, 2020, S.&#xA0;1695&#x2013;1704</div>
</div>
<div id="ref-mensch2018" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[MeBl18] </div><div class="csl-right-inline"><span class="smallcaps">Mensch, Arthur</span> ; <span class="smallcaps">Blondel, Mathieu</span>: Differentiable Dynamic Programming for Structured Prediction and Attention. In: <em>Proceedings of the International Conference on Machine Learning</em>, <em>Proceedings of Machine Learning Research</em>. Bd. 80&#xA0;: PMLR, 2018, S.&#xA0;3462&#x2013;3471</div>
</div>
</div>
</body>
</html>
